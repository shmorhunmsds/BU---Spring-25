{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Overview\n",
    "This week, we will build on our knowledge of relational databases by focusing on database development and querying. We’ll explore how to design efficient database structures and write structured query language (SQL) queries to retrieve and optimize data effectively.\n",
    "\n",
    "## Learning Objectives\n",
    "At the end of this week, you will be able to: \n",
    "- Outline the data normalization process \n",
    "- Recall the fundamentals of database development \n",
    "- Describe query behavior and best practices to ensure performant queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Overview: Data Normalization Process\n",
    "In this section, we explore the principles and techniques of data normalization, focusing on its objectives and the importance of organizing data effectively. We examine different normal forms and walk through practical steps to normalize a database, ensuring efficient data structure and integrity. Finally, we balance normalization with denormalization to optimize performance, looking at how we transform raw data into third normal form (3NF).\n",
    "\n",
    "## 1.1 Lesson: Data Normalization\n",
    "Imagine you’re managing a customer database for an online store. Each time a customer places an order, their name, address, and payment details are stored alongside the order information. At first, this seems fine—until a customer updates their address. Now you’re faced with a problem: Their old address is still attached to previous orders, leading to potential shipping errors. Worse, any change in payment details must be updated in multiple places, increasing the risk of inconsistencies or missed updates. \n",
    "\n",
    "How can you structure your database to store each piece of information in one place, ensuring accuracy, consistency, and easier updates? This is where normalization comes in.  \n",
    "\n",
    "**Data normalization** is the process of organizing data in a relational database to minimize redundancy and dependency by dividing large tables into smaller, related tables and defining relationships between them. It follows a series of rules, known as normal forms, that guide how data should be structured to eliminate inconsistencies, maintain data integrity, and improve query efficiency. The primary goals of normalization are to reduce duplicate data, ensure accuracy, and create a clean, scalable database schema. \n",
    "\n",
    "In the following video, we explore data normalization—a key process in relational database design. You'll learn what normalization is, why it’s essential, and how it eliminates redundancy, ensures data integrity, and optimizes query performance. We’ll also break down the normal forms, from first normal form (1NF) to 3NF, to understand how they address redundancy and improve database structure. Mastering normalization allows you to design efficient, scalable, performant, and maintainable database systems.\n",
    "\n",
    "- Data normalization is a crucial process in relational database design that helps eliminate data redunancy, ensure data integrity, and improve query efficiency.\n",
    "- Normalization\n",
    "    - Reduces redunancy by dividing large tables into small related tables.\n",
    "    - Normal forms guide how data is structured\n",
    "- Data Normalization Objectives\n",
    "    1. Eliminate data redundancy\n",
    "    2. Ensure data integrity and consistency\n",
    "    3. Simplify database maintenance\n",
    "    4. Optimize query performance\n",
    "\n",
    "\n",
    "### Discuss Data Normalization and Its Objectives\n",
    "As discussed, **data normalization** is a fundamental process in relational database design that aims to reduce redundancy, ensure data integrity, and enhance query performance. We discussed how normalization organizes data by dividing large tables into smaller, related ones based on rules called **normal forms (1NF to 5NF)**. These forms address specific types of redundancy and anomalies, with a focus on maintaining atomic values, removing partial and transitive dependencies, and ensuring relationships are properly structured. \n",
    "\n",
    "We also discussed how the objectives of normalization include the elimination of **data redundancy**, ensuring **data integrity and consistency** through the use of primary and foreign keys, simplifying **database maintenance** by making schema changes easier, and optimizing **query performance** by structuring data logically. It is important to understand normalization is crucial for creating efficient, scalable, and reliable database systems, making it a cornerstone of relational database design.\n",
    "\n",
    "#### Normal Forms \n",
    "$$\\mathbf{1NF} \\quad \\rightarrow \\quad \\mathbf{2NF} \\quad \\rightarrow \\quad \\mathbf{3NF} \\quad \\rightarrow \\quad \\mathbf{4NF} \\quad \\rightarrow \\quad \\mathbf{5NF}$$\n",
    "$$ \\text{fewer constraints} \\quad \\quad \\quad \\quad  \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\text{more constraints}$$\n",
    "$$ \\text{less data integrity} \\quad \\quad \\quad \\quad  \\quad \\quad  \\quad \\quad \\quad \\quad \\text{more data integrity}$$\n",
    "\n",
    "| 1NF | 2NF | 3NF | 4NF | 5NF |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| Each table has a primary key. All Attributes contain atomic values (no repeating groups or arrays). | Removes partial dependencies. Ensures all non-key attributes fully depend on primary key. | Eliminates transitive dependencies. ensures non-key attributes are not dependent on other non-key attribute. | Address multi-valued dependencies and complex relationships. | address multi-valued dependencies and complex relationships. |\n",
    "\n",
    "___\n",
    "\n",
    "### Resources | MSFT Description of the database normalization basics | https://learn.microsoft.com/en-us/office/troubleshoot/access/database-normalization-description\n",
    "\n",
    "#### First Normal Form\n",
    "- Eliminate repeating groups in individual tables\n",
    "- create a separate table for each set of related data\n",
    "- identify each set of related data with a primary key\n",
    "\n",
    "Don't use multiple fields in a single table to store similar data. For example, to track an inventory item that may come from two possible sources, an inventory record may contain fields for `Vendor Code 1` and `Vendor Code 2`.\n",
    "\n",
    "What happens when you add a third vendor? Adding a field isn't the answer; it requires program and table modifications and doesn't smoothly accommodate a dynamic number of vendors. Instead, place all vendor information in a separate table called Vendors, then link inventory to vendors with an item number key, or vendors to inventory with a vendor code key.\n",
    "\n",
    "#### Second Normal Form\n",
    "- Create tables for sets of values that apply to multiple records.\n",
    "- Relate these tables with a foreign key\n",
    "\n",
    "Records shouldn't depend on anything other than a table's primary key (a compound key, if necessary). \n",
    "- For example, consider a customer's address in an accounting system. The address is needed by the Customers table, but also by the Orders, Shipping, Invoices, Accounts Receivable, and Collections tables. Instead of storing the customer's address as a separate entry in each of these tables, store it in one place, either in the Customers table or in a separate Addresses table.\n",
    "\n",
    "#### Third Normal Form\n",
    "\n",
    "- Eliminates fields that don't depend on the key\n",
    "\n",
    "Values in a record that aren't part of that record's key don't belong in the table. In general, anytime the contents of a group of fields may apply to more than a single record in the table, consider placing those fields in a separate table.\n",
    "\n",
    "- For example, in an Employee Recruitment table, a candidate's university name and address may be included. But you need a complete list of universities for group mailings. If university information is stored in the Candidates table, there is no way to list universities with no current candidates. Create a separate Universities table and link it to the Candidates table with a university code key.\n",
    "- EXCEPTION: Adhering to the third normal form, while theoretically desirable, isn't always practical. If you have a Customers table and you want to eliminate all possible interfield dependencies, you must create separate tables for cities, ZIP codes, sales representatives, customer classes, and any other factor that may be duplicated in multiple records. In theory, normalization is worth pursuing. However, many small tables may degrade performance or exceed open file and memory capacities.\n",
    "\n",
    "It may be more feasible to apply third normal form only to data that changes frequently. If some dependent fields remain, design your application to require the user to verify all related fields when any one is changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing an Example Table\n",
    "These steps demonstrate the process of normalizing a fictitious student table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student#</th>\n",
       "      <th>Advisor</th>\n",
       "      <th>Adv-Room</th>\n",
       "      <th>Class1</th>\n",
       "      <th>Class2</th>\n",
       "      <th>Class3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1022</td>\n",
       "      <td>Jones</td>\n",
       "      <td>412</td>\n",
       "      <td>101-07</td>\n",
       "      <td>143-01</td>\n",
       "      <td>159-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4123</td>\n",
       "      <td>Smith</td>\n",
       "      <td>216</td>\n",
       "      <td>101-07</td>\n",
       "      <td>143-01</td>\n",
       "      <td>179-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student# Advisor  Adv-Room  Class1  Class2  Class3\n",
       "0      1022   Jones       412  101-07  143-01  159-02\n",
       "1      4123   Smith       216  101-07  143-01  179-04"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "unnormalized = {'Student#': [1022, 4123],\n",
    "                'Advisor' : ['Jones', 'Smith'],\n",
    "                'Adv-Room' : [412, 216],\n",
    "                'Class1' : ['101-07', '101-07'],\n",
    "                'Class2' : ['143-01', '143-01'],\n",
    "                'Class3' : ['159-02', '179-04'],}\n",
    "unnormalized_df = pd.DataFrame(unnormalized)\n",
    "unnormalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First normal form: No repeating groups\n",
    "- Tables should have only two dimensions. Since one student has several classes, these classes should be listed in a separate table. Fields Class1, Class2, and Class3 in the above records are indications of design trouble.\n",
    "- Spreadsheets often use the third dimension, but tables shouldn't. Another way to look at this problem is with a one-to-many relationship, don't put the one side and the many sides in the same table. Instead, create another table in first normal form by eliminating the repeating group (Class#), as shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student#</th>\n",
       "      <th>Advisor</th>\n",
       "      <th>Adv-Room</th>\n",
       "      <th>Class</th>\n",
       "      <th>Course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1022</td>\n",
       "      <td>Jones</td>\n",
       "      <td>412</td>\n",
       "      <td>Class1</td>\n",
       "      <td>101-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1022</td>\n",
       "      <td>Jones</td>\n",
       "      <td>412</td>\n",
       "      <td>Class2</td>\n",
       "      <td>143-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1022</td>\n",
       "      <td>Jones</td>\n",
       "      <td>412</td>\n",
       "      <td>Class3</td>\n",
       "      <td>159-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4123</td>\n",
       "      <td>Smith</td>\n",
       "      <td>216</td>\n",
       "      <td>Class1</td>\n",
       "      <td>101-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4123</td>\n",
       "      <td>Smith</td>\n",
       "      <td>216</td>\n",
       "      <td>Class2</td>\n",
       "      <td>143-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4123</td>\n",
       "      <td>Smith</td>\n",
       "      <td>216</td>\n",
       "      <td>Class3</td>\n",
       "      <td>179-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student# Advisor  Adv-Room   Class  Course\n",
       "0      1022   Jones       412  Class1  101-07\n",
       "2      1022   Jones       412  Class2  143-01\n",
       "4      1022   Jones       412  Class3  159-02\n",
       "1      4123   Smith       216  Class1  101-07\n",
       "3      4123   Smith       216  Class2  143-01\n",
       "5      4123   Smith       216  Class3  179-04"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unnormalized_df_melt = pd.melt(unnormalized_df, id_vars=['Student#', 'Advisor', 'Adv-Room'], value_vars=['Class1', 'Class2', 'Class3'], var_name='Class', value_name='Course')\n",
    "unnormalized_df_melt.sort_values(by='Student#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Normal Form\n",
    "Second normal form: Eliminate redundant data\n",
    "- Note the multiple Class# values for each Student# value in the above table. Class# isn't functionally dependent on Student# (primary key), so this relationship isn't in second normal form.\n",
    "\n",
    "The following tables demonstrate second normal form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student#</th>\n",
       "      <th>Advisor</th>\n",
       "      <th>Adv-Room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1022</td>\n",
       "      <td>Jones</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4123</td>\n",
       "      <td>Smith</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student# Advisor  Adv-Room\n",
       "0      1022   Jones       412\n",
       "1      4123   Smith       216"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_df = unnormalized_df_melt[['Student#', 'Advisor', 'Adv-Room']].drop_duplicates()\n",
    "students_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student#</th>\n",
       "      <th>Course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1022</td>\n",
       "      <td>101-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4123</td>\n",
       "      <td>101-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1022</td>\n",
       "      <td>143-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4123</td>\n",
       "      <td>143-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1022</td>\n",
       "      <td>159-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4123</td>\n",
       "      <td>179-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student#  Course\n",
       "0      1022  101-07\n",
       "1      4123  101-07\n",
       "2      1022  143-01\n",
       "3      4123  143-01\n",
       "4      1022  159-02\n",
       "5      4123  179-04"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registration = unnormalized_df_melt[['Student#', 'Course']]\n",
    "registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third normal form: Eliminate data not dependent on key\n",
    "\n",
    "In the last example, Adv-Room (the advisor's office number) is functionally dependent on the Advisor attribute. The solution is to move that attribute from the Students table to the Faculty table, as shown below:\n",
    "\n",
    "Students:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student#</th>\n",
       "      <th>Advisor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1022</td>\n",
       "      <td>Jones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4123</td>\n",
       "      <td>Smith</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student# Advisor\n",
       "0      1022   Jones\n",
       "1      4123   Smith"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_df_3rd = students_df[['Student#', 'Advisor']]\n",
    "students_df_3rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Advisor</th>\n",
       "      <th>Adv-Room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jones</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Smith</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Advisor  Adv-Room\n",
       "0   Jones       412\n",
       "1   Smith       216"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faculty_df = unnormalized_df_melt[['Advisor', 'Adv-Room']].drop_duplicates()\n",
    "faculty_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Lesson: How to Normalize a Database\n",
    "This lesson will discuss how normalizing a database is the process of organizing its structure to reduce redundancy and improve data integrity. This involves breaking down large, complex tables into smaller, more focused tables and defining clear relationships between them. By following a series of steps based on normalization rules, or normal forms, you ensure that data is stored efficiently and consistently. The goal is to eliminate duplicate data, minimize dependencies, and create a database structure that is easy to maintain and scalable for future growth.\n",
    "\n",
    "### Demonstration of Walk-through from Raw Data to 3NF\n",
    "To get started on how to normalize a specific dataset you are given with the goal of designing a database, you’ll first need to assess the data you are working with. From there, the process of normalizing a database follows a series of structured steps called normal forms, with each step addressing specific types of data redundancy and anomalies. Here’s an overview: \n",
    "\n",
    "1. **First normal form (1NF):** This step ensures that all data is stored in a tabular format with rows and columns. Each column must contain atomic (indivisible) values, and repeating groups or arrays within columns must be removed. A primary key is defined to uniquely identify each row. \n",
    "2. **Second normal form (2NF):** Building on 1NF, this step eliminates **partial dependencies**, ensuring that all non-key attributes are fully dependent on the entire primary key. This is especially important for tables with composite primary keys, where attributes may depend on only part of the key. \n",
    "3. **Third normal form (3NF):** In this step, **transitive dependencies** are removed. A non-key attribute should not depend on another non-key attribute. Instead, attributes must depend only on the primary key, ensuring the table’s integrity and eliminating unnecessary relationships. \n",
    "4. **Fourth normal form (4NF):** 4NF addresses **multi-valued dependencies**, where one attribute depends on multiple independent values of another attribute. By separating such data into different tables, this step ensures that each table represents one independent concept or relationship. \n",
    "5. **Fifth normal form (5NF):** The final step deals with **complex join dependencies** by breaking down tables to eliminate redundancy caused by multi-table relationships. Each piece of data is stored in its simplest form, ensuring that no information is lost when the tables are joined. \n",
    "\n",
    "Through these steps, normalization transforms a database into a clean, efficient, and logically structured system that minimizes redundancy, maintains data integrity, and supports scalability. Most practical applications focus on achieving up to **3NF**, as it typically balances efficiency and complexity for most use cases.  \n",
    "\n",
    "Something to be mindful of is the possibility of over-normalizing a database so it leads to less efficiency. In the next lesson, we will look at how to balance whether we are over-normalizing or need to denormalize afterward for analytical or other uses. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Normal Form Rules\n",
    "1. Using row order to convey information is not permitted\n",
    "2. Mixing data types within the same column is not permitted\n",
    "3. Having a table without a primary key is not permitted.\n",
    "4. Repeating groups are not permitted\n",
    "\n",
    "#### Second Normal Form (2NF)\n",
    "1. Each non-key attribute must depend on the entire primary key\n",
    "\n",
    "#### Third Normal Form (3NF)\n",
    "Each non-key attribute must depend on the key, the whole key, and nothing but the key\n",
    "\n",
    "#### Boyce-Codd Normal Form (BCNF)\n",
    "each attribute in the table must depend on the key, the whole key, and nothing but the key\n",
    "\n",
    "#### Fourth Normal FOrm (4NF)\n",
    "The only kinds of multivalued dependency allowed in a table are multivalued dependencies on the key.\n",
    "\n",
    "#### Fifth Normal Form\n",
    "It must not be possible to describe the table as being the logical result of joining some other tables together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Lesson: Balancing Normalization and Denormalization\n",
    "Balancing normalization and denormalization is key to designing a database that optimizes both performance and data integrity. While normalization reduces redundancy and ensures data consistency, denormalization reintroduces controlled redundancy to enhance query performance. Here’s a step-by-step guide to achieving this balance:\n",
    "\n",
    "1. **Understand the Business Needs**\n",
    "    - Start by analyzing the business requirements and identifying the types of queries and reports the database needs to support. \n",
    "    - Determine the priorities: Is data consistency more critical or is query performance the main concern?\n",
    "2. **Normalize to the Necessary Level**\n",
    "    - Begin with normalization to at least **3NF**, ensuring that the database structure minimizes redundancy, maintains data integrity, and avoids anomalies. \n",
    "    - For more complex requirements, consider additional normalization (e.g., up to 4NF or 5NF) but balance this with the practicality of implementation.\n",
    "3. **Identify Performance Bottlenecks**\n",
    "    - Use query profiling and performance monitoring tools to evaluate the database after initial normalization. \n",
    "    - Identify queries that require multiple joins or those that perform slowly due to over-normalized structures.\n",
    "4. **Strategically Denormalize**\n",
    "    - Introduce denormalization selectively for specific use cases where performance improvements outweigh the risk of redundancy. Examples include: \n",
    "        - **Adding summary tables** by pre-aggregating data for faster reporting. \n",
    "        - **Duplicating frequently accessed columns**, reducing the need for joins in high-traffic queries. \n",
    "        - **Combining related tables** by merging tables accessed together frequently to streamline retrieval. \n",
    "        - **Document and control redundant data** to ensure that updates remain consistent across the database.\n",
    "5. **Leverage Indexing and Caching**\n",
    "    - Use indexing to speed up data retrieval without heavily denormalizing. Primary keys, foreign keys, and frequently queried columns are good candidates for indexing. \n",
    "    - Employ caching solutions to handle repeated queries, reducing the need to reintroduce redundancy.\n",
    "6. **Consider Partitioning**\n",
    "    - For large datasets, horizontal or vertical partitioning can improve performance without compromising normalization. Partitioning allows you to focus queries on smaller, relevant data subsets.\n",
    "7. **Test and Iterate**\n",
    "    - Continuously test the performance of the database after introducing denormalization. Ensure that the added redundancy does not compromise data accuracy or update processes. \n",
    "    - Adjust the balance as the application evolves, accommodating changes in query patterns or data volume.\n",
    "8. **Maintain Data Integrity**\n",
    "    - Implement automated processes or triggers to ensure redundant data remains consistent when updated. \n",
    "    - Use constraints and referential integrity checks to mitigate risks introduced by denormalization.\n",
    "\n",
    "Balancing normalization and denormalization is an iterative process. Start with a fully normalized structure to maintain integrity, then selectively denormalize where performance improvements are necessary. By monitoring and refining the database design, you can achieve an optimal balance that meets both performance and consistency requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Check 1\n",
    "1. **Q - A retail company needs to store customer data, including their name, address, and multiple phone numbers. In a normalized database, how should this be structured to meet the requirements of the relational model?**\n",
    "    - **A: Use a customer table for name and address and a separate table for phone numbers, linked by a customer ID.**\n",
    "        - Storing phone numbers in a separate table eliminates multivalued attributes and complies with 1NF, ensuring each value is atomic and properly related. \n",
    "2. **Q - A healthcare system has a database where appointments are linked to patients. The system also stores appointment types, such as check-ups or follow-ups. To improve performance, the team combines the appointment and appointment type tables into one table. What risk does this denormalization introduce?**\n",
    "    - **A: Increased redundancy and potential data inconsistencies.**\n",
    "        - Denormalization by merging the appointment and appointment type tables introduces redundancy, which could lead to inconsistencies when data is updated.\n",
    "3. **Q - An e-commerce company uses a database to track orders and items. The normalized structure requires multiple joins to generate reports. To improve query performance, the company creates a summary table combining order details and item data. What should it do to maintain data integrity after denormalization?**\n",
    "    - **A: Use triggers or scripts to update the summary table when order or item data changes.**\n",
    "        - After denormalization, automated processes like triggers ensure consistency between the summary table and the original tables, maintaining data integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Overview: Fundamentals of Relational Database Management\n",
    "In this section, we review the core principles of relational database design, focusing on how database elements and data types work together. We examine tables, keys, and relationships to understand how they define and connect data within a database. Finally, we analyze the impact of data types on database design to ensure efficient and accurate data management. \n",
    "\n",
    "### Learning Objectives \n",
    "- Recall the fundamentals of database development\n",
    "\n",
    "## 2.1 Lesson: Relational Database Design\n",
    "In this video, we cover the core principles of relational database design, including structuring data into tables, defining primary and foreign keys, applying constraints, and using normalization techniques. We explore how to create efficient, scalable, and maintainable databases that meet business needs.\n",
    "\n",
    "### Core Principles of Relational Database Design and Understanding Data Types\n",
    "Relational databases should be efficient, scalable, and maintainable.\n",
    "\n",
    "#### Designing a Relational Database\n",
    "1. Undestand its purpose and scope. Does your design align with business requirements and future scalability needs?\n",
    "2. Relational tables should represent real-world entities.\n",
    "3. Primary keys uniquely identify each record in a table, and foreign keys establish relationships between tables.\n",
    "4. Use normalization techniques to eliminate redunancy and ensure each item is stored only once.\n",
    "5. Enforce data integrity by applying constraints:\n",
    "    - Uniqueness as constraint to ensure data cannot be duplicated.\n",
    "    - Not null constraint requires data to be added to field.\n",
    "    - Primary key and foreign keys ensure efficient relationships between tables.\n",
    "\n",
    "As discussed, relational databases form the backbone of modern data management systems, providing structured, reliable, and scalable solutions for storing and retrieving data. Understanding the fundamentals of relational database development is essential for designing systems that meet business needs while ensuring data integrity and efficiency. This exploration includes key principles of relational database design and an in-depth look at data types, both of which significantly impact how databases are structured and perform. \n",
    "\n",
    "Relational database design revolves around creating a structure that organizes data efficiently, reduces redundancy, and ensures accuracy. Key principles include: \n",
    "\n",
    "- **Normalization** \n",
    "    - Organize data into tables to reduce redundancy and dependency.\n",
    "    - Follow normal forms (1NF to 3NF and beyond if necessary) to ensure consistency and integrity. \n",
    "- **Keys and Relationships** \n",
    "    - Define **primary keys** for unique identification of records. \n",
    "    - Use **foreign keys** to establish relationships between tables, ensuring referential integrity. \n",
    "    - Design tables with appropriate relationships (one-to-one, one-to-many, many-to-many). \n",
    "- **Data Integrity** \n",
    "    - Enforce constraints (e.g., `NOT NULL`, `UNIQUE`) to ensure data accuracy and reliability. \n",
    "    - Use transaction management to adhere to ACID properties (atomicity, consistency, isolation, durability). \n",
    "- **Scalability and Performance** \n",
    "    - Structure the database to handle current and future data volume efficiently. \n",
    "    - Use indexing and query optimization techniques to improve data retrieval times. \n",
    "\n",
    "By following these best practices, although they are not a perfect solution, you will have a great starting point for ensuring an efficiently managed database solution. Be mindful, however, that as your database grows and new needs arise, certain situations may not be accounted for, and some flexibility or even possible redesign will be needed.\n",
    "\n",
    "## 2.2 Lesson: Impact of Data Types on Database Design\n",
    "Now let’s take a look at the importance of choosing the right data types for your database design. In the following video, we explore how data types influence storage efficiency, query performance, and data integrity. You’ll learn how to select the most appropriate data types, from numerical and string types to date and time fields, for your columns and understand the impact they have on the overall functionality and performance of your database.\n",
    "\n",
    "### Choosing the Right Data Types for Your Data\n",
    "Choosing the data types for your attributes is another critical aspect to consider. Data types define the types of values that can be stored in a column and influence storage requirements, query performance, and data integrity. \n",
    "\n",
    "#### Data Types\n",
    "- Integer and Floating Point Data Types: Storing Numerical Values. \n",
    "    - Integers are ideal for discrete values like counts or IDs.\n",
    "    - Floating points are ideal for continuous values, like prices or measurements.\n",
    "- Character and String Data Types: Textual Information\n",
    "    - Use `VARCHAR` for variable length text, such as customer names or descriptions.\n",
    "- Date and Time Data Types: Temporal Data, like timestamps and durations\n",
    "    - Using date data types ensures proper sorting and filtering of time related data.\n",
    "\n",
    "#### Impact on Performance and Storage\n",
    "- Using `VARCHAR (100)` instead of `TEXT` for short strings conserves space and speeds for queries.\n",
    "- Using `DATE` instead of `DATETIME` for date-only storage reduces storage requirements.\n",
    "\n",
    "Data types define the kind of data that can be stored in a column, impacting both storage requirements and query performance. Choosing appropriate data types is crucial for optimizing database performance and ensuring data accuracy. Key considerations include: \n",
    "\n",
    "- Basic Data Types: \n",
    "    - Numeric (e.g., INT, FLOAT): For storing numbers, use precise types based on the nature of calculations. \n",
    "    - Character (e.g., CHAR, VARCHAR): For text storage, choose between fixed or variable lengths, depending on data variability. \n",
    "    - Date/Time: For storing timestamps and durations, enabling temporal analysis. \n",
    "- Impact on Storage: \n",
    "    - Larger data types consume more storage, impacting disk space and performance. \n",
    "    - Choosing overly precise data types can lead to inefficiencies, while underestimating precision may lead to data loss. \n",
    "- Performance Implications: \n",
    "    - Indexed columns should use data types that allow for efficient comparison operations. \n",
    "    - Joins and filtering operations are faster with consistent data types across related tables. \n",
    "- Specialized Data Types: \n",
    "    - Use JavaScript object notation (JSON) or XML for semi-structured data. \n",
    "    - Leverage binary large objects (BLOBs) for storing binary data like images or videos. \n",
    "\n",
    "Although this is an abbreviated list of the various data types and their functions, as your database design acumen improves, you will be able to better evaluate the proper data type for your specific use case. Note that with Homework 1c, you’ll be using the simple “date” data type and not “datetime,” as there is a distinction between the two. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Check 2:\n",
    "1. Q - A company is designing a database to track employee records. Each employee must have a unique identifier, and their records should not be duplicated. What is the most appropriate way to implement this in the database?\n",
    "    - A -  Assign a primary key to the employee ID column\n",
    "    - A primary key ensures each employee record is unique and acts as a unique identifier for each entry in the database.\n",
    "2. Q - A healthcare database needs to store patient names, ages, and appointment dates. Which data types would best suit these columns?\n",
    "    - A - VARCHAR for names, INT for ages, and DATE for appointment dates\n",
    "    - VARCHAR is suitable for variable-length text like names, INT is ideal for storing whole numbers like ages, and DATE is specifically designed for storing dates.\n",
    "3. Q - A retail company wants to create a database to track sales. To avoid redundant data, the company separates customer information, product details, and sales records into different tables. What principle is the company applying?\n",
    "    - A - Normalization\n",
    "    - Normalization involves organizing data into separate tables to reduce redundancy and ensure logical relationships between data.\n",
    "4. Q - A financial services database has a column for transaction amounts. The column needs to store precise monetary values with up to two decimal places. Which data type should be used?\n",
    "    - A - DECIMAL\n",
    "    - The DECIMAL types is specifically designed for precise numerical cvalues, making it ideal for monetary amounts that require fixed precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Overview: Query Behavior\n",
    "In this section, we review SQL, exploring its query behavior, basic syntax, and how execution plans work. We dive into indexing strategies to enhance query performance and examine common query optimization techniques to improve database efficiency. Through these lessons, we build a solid foundation for writing and optimizing SQL queries.  \n",
    "\n",
    "### Learning Objectives \n",
    "- Describe query behavior and best practices to ensure performant queries\n",
    "\n",
    "## 3.1 Lesson: Introduction to SQL\n",
    "\n",
    "### Review Basic SQL Concepts and Syntax\n",
    "\n",
    "SQL Operations\n",
    "- Creating\n",
    "- Reading\n",
    "- Updating\n",
    "- Deleting\n",
    "\n",
    "Also known as **CRUD** operations\n",
    "\n",
    "3 Key SQL Concepts\n",
    "1. Tables and Records\n",
    "2. SQL Statements\n",
    "3. SQL Clauses\n",
    "\n",
    " \n",
    "### Table 1 | *Fundamental SQL Concepts Essential for Development and Querying*\n",
    "\n",
    "| SQL Concept | Purpose | Key Commands and Features |\n",
    "| :--- | :--- | :--- |\n",
    "| Data definition language (DDL) | Purpose: Used to define and manage the structure of data base objects such as tables,  indexes, and schemas. | Key Commands:</li> <ul><li>CREATE: Defines new database objects (e.g., tables, indexes).</li><li>ALTER: Modifies the structure of existing objects (e.g., add/drop columns)<li>DROP: Deletes database objects permanently.</li></ul>|\n",
    "| Data Manipulation Language (DML) | Purpose: Allows developers to insert, update, delete, and retrieve data. |  Key Commands:</li> <ul><li>INSERT: Adds new rows to a table. </li><li>UPDATE: Modifies existing rows. <li> DELETE: Removes rows from a table. <li> SELECT: Retreives data from one or more tables. </li></ul> |\n",
    "| Data Querying with SELECT Statements | Purpose: Filter, sort, aggregate, and group results. | Key Features :</li> <ul><li>Filtering data: Use WHERE to specify conditions (e.g., WHERE age > 30).</li><li>Sorting results: Use ORDER BY to sort the output by one or more columns. <li> Aggregating data: Functions like COUNT, SUM, AVG, MIN, and MAX allow summarizing data. <li> Grouping results: Use GROUP BY to group rows sharing the same values in specific columns. <li> Filtering groups: Use HAVING to filer groups created by GROUP BY. </li></ul> |\n",
    "| Data relationships and joins | Purpose: Relational databases use tables linked by keys. Joins retrieve related data across multiple tables. | Key Commands:</li> <ul><li> INNER JOIN: Returns matching rows in both tables..</li><li>LEFT JOIN (or LEFT OUTER JOIN): Returns all rows from the left table and matching rows from the right table. <li> RIGHT JOIN: Returns all rows from the right table and matching rows from the left table. <li> FULL JOIN: Combines results of both LEFT AND RIGHT JOIN. </li></ul>|\n",
    "| Indexing and query optimization | Purpose of Indexing: Speeds Up data Retrieval by creating a structure that allows the database to locate data quickly. | <ul><li> Usage: Apply indexes to columns frequently used in WHERE, JOIN, or ORDER BY clauses. </li></ul> |\n",
    "| SQL Constraints | Purpose: Ensure data accuracy and integrity. | Key Commands: </li> <ul><li> PRIMARY KEY: Uniquely identifies each row in a table.  </li><li> FOREIGN KEY: Ensures referential integrity between tables. </li><li> UNIQUE: Ensures all values in a column are distinct. </li><li> NOT NULL: Ensures a column cannot have NULL values. </li></ul> |\n",
    "\n",
    "### Basic SQL Best Practices \n",
    "- Avoid SELECT *; specify only the columns needed. \n",
    "- Use parameterized queries to prevent SQL injection. \n",
    "- Regularly analyze execution plans to optimize queries. \n",
    "- Leverage indexing but avoid over-indexing to maintain efficient write operations. \n",
    "\n",
    "Mastering these fundamental SQL concepts enables developers to design robust relational databases and write efficient queries tailored to their applications. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Lesson: Indexing Strategies\n",
    "\n",
    "Indexing in databases is a powerful tool for improving query performance by allowing users to locate and retrieve data faster. Effective indexing strategies focus on aligning indexes with query patterns and minimizing performance overhead. Key points include: \n",
    "\n",
    "- Choosing the Right Columns for Indexing \n",
    "    - Index columns frequently used in WHERE, JOIN, or ORDER BY clauses. \n",
    "    - Avoid indexing columns with low cardinality (e.g., binary or Boolean fields) unless critical for specific queries. \n",
    "- Using Composite Indexes \n",
    "    - Create composite indexes for queries involving multiple columns but order columns based on their usage in filters and sorting. \n",
    "    - For example, if a query filters by state and then sorts by city, the composite index should be (state, city). \n",
    "- Covering Indexes \n",
    "    - Use indexes that include all columns referenced in a query to reduce the need to access the main table (index-only scans). \n",
    "- Avoid Over-Indexing \n",
    "    - Too many indexes can degrade performance due to increased overhead during INSERT, UPDATE, or DELETE operations. Regularly monitor and remove unused indexes. \n",
    "- Index Maintenance \n",
    "    - Rebuild or reorganize fragmented indexes periodically, especially for heavily used or updated tables. \n",
    "\n",
    "Best practices include analyzing query execution plans to validate the effectiveness of indexing and balancing the benefits of faster reads with the potential costs on write performance. Each database management system (DBMS) has different ways to analyze the query execution plans for your database, so familiarize yourself with these techniques as you are designing your database for optimal performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Lesson: Query Optimization Techniques\n",
    "\n",
    "Query optimization techniques aim to refine SQL queries to reduce execution time and resource consumption. These techniques are closely tied to query behavior and database best practices: \n",
    "\n",
    "- Writing Selective Queries \n",
    "    - Use precise filtering conditions in WHERE clauses to limit the rows processed. \n",
    "    - Avoid using SELECT *; specify only the necessary columns to reduce data retrieval. \n",
    "- Leverage Joins and Subqueries Efficiently \n",
    "    - Use INNER JOIN instead of OUTER JOIN when possible, as it processes fewer rows. \n",
    "    - Optimize subqueries by replacing them with joins or common table expressions (CTEs) when they improve clarity and performance. \n",
    "- Avoid Functions on Indexed Columns \n",
    "    - Avoid wrapping indexed columns in functions (e.g., WHERE UPPER(column) = 'VALUE'), as this disables index usage. \n",
    "    - Instead, preprocess inputs to match the column format. \n",
    "- Minimize Data Movement \n",
    "    - Use filtering conditions as early as possible in the query to reduce data processed in later steps. \n",
    "    - Employ pagination for large result sets using LIMIT or OFFSET to avoid retrieving unnecessary rows. \n",
    "- Analyze Query Execution Plans \n",
    "    - Regularly review execution plans to identify bottlenecks, such as full table scans or inefficient joins. \n",
    "    - Adjust indexes, query structure, or database configuration based on the analysis. \n",
    "- Caching and Materialized Views \n",
    "    - Cache results of complex queries that do not change frequently. \n",
    "    - Use materialized views to precompute and store results for faster access. \n",
    "\n",
    "Adhering to these techniques ensures that queries are performant, reduce unnecessary resource consumption, and support scalable database operations. These thoughts should be kept in mind as we continue into our use of SQL and advanced techniques in next week’s lessons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Check 3\n",
    "\n",
    "1. Q - A financial database contains a Transactions table with millions of rows. A query retrieves all transactions above $1,000,000 using WHERE transaction_amount > 1000000. What is the best practice to optimize this query?\n",
    "- A - Correct: Create an index on the transaction_amount column.\n",
    "- Indexing the transaction_amount column allows the database to quickly locate rows matching the condition, improving query performance.\n",
    "\n",
    "2. Q - A user frequently filters and sorts a Products table by category and price. What indexing strategy would provide the most efficient query performance?\n",
    "- A - Correct: Create a composite index on category and price.\n",
    "- A composite index on category and price is more efficient for queries that filter by category and sort by price than separate indexes.\n",
    "\n",
    "3. Q - A database query involves multiple joins and filters, and its execution time is high. What is the first step in diagnosing and improving its performance?\n",
    "- A - Correct: Examine the query’s execution plan to identify bottlenecks.\n",
    "- Examining the execution plan helps identify specific bottlenecks, such as unnecessary table scans or inefficient joins, providing a targeted approach for optimization.\n",
    "\n",
    "4. Q - A reporting query retrieves large datasets, but users only need to view 20 results at a time. Which optimization technique should be applied?\n",
    "- A - Correct: Implement pagination with LIMIT and OFFSET.\n",
    "- Pagination with LIMIT and OFFSET retrieves only the required subset of data, reducing the query’s resource consumption and improving performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
