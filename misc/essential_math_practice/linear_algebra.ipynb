{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Vector?\n",
    "- A vector is an arrow in space with a specific direction and length, often representing a piece of data. It is the central building block of linear algebra, including matrices and linear transformations. The purpose of a vector is to visually represent a piece of data.  We declare a vector mathematically like this:\n",
    "\n",
    "$ \\vec{v} \\quad = \\quad \\begin{bmatrix} x \\\\ y \\end{bmatrix}$\n",
    "\n",
    "$ \\vec{v} \\quad = \\quad \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}$ \n",
    "\n",
    "we can declare a vector using a list or a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2]\n"
     ]
    }
   ],
   "source": [
    "v = [3,2]\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "v = np.array([3,2])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors can exist on more than two dimensions, next we declare a three-dimensional vector along axes x, y, and z:\n",
    "\n",
    "$ \\vec{v} \\quad = \\quad \\begin{bmatrix} x \\\\ y  \\\\ z \\end{bmatrix} \\quad = \\quad \\begin{bmatrix} 4 \\\\ 1  \\\\ 2 \\end{bmatrix}$\n",
    "\n",
    "Expressed using numpy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 2]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([4, 1, 2])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# 5D vector\n",
    "v = np.array([1, 2, 3, 4, 5])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding and combining vectors\n",
    "\n",
    "say we have two vectors $\\vec{v}$ and $\\vec{w}$. How do we add those two vectors together?\n",
    "\n",
    "$\\vec{v} \\quad = \\quad \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}$\n",
    "\n",
    "$\\vec{w} \\quad = \\quad \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}$\n",
    "\n",
    "$\\vec{v} + \\vec{w} \\quad = \\quad \\begin{bmatrix} 3 + 2 \\\\ 2 + -1 \\end{bmatrix} \\quad = \\quad \\begin{bmatrix} 5 \\\\ 1 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "v = array([3,2])\n",
    "w = array([2, -1])\n",
    "\n",
    "v_plus_w = v + w\n",
    "print(v_plus_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Vectors\n",
    "\n",
    "*Scaling* is growing or shrinking a vectors length.  You can grow/shrink a vector by multiplying or scaling it with a single value, known as a *scalar*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 2]\n"
     ]
    }
   ],
   "source": [
    "v = array([3, 1])\n",
    "scaled_v  = 2 * v\n",
    "print(scaled_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling a vector does not change its direction, only its magnitude. When you multiply a vector by a negative number it flips the direction of the vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Span and Linear Dependence\n",
    "\n",
    "With these two operations (adding and scaling vectors), we can combine two vectors and scale them to create any resulting vector that we want.\n",
    "- The whole space of possible vectors is called *span*. and in most cases our span can create unlimited vectors off those two vectors, simply by scaling and summing them.\n",
    "- When we have two vectors in two different directions,  they are *linearly independent* and have this unlimited span.\n",
    "\n",
    "But what happens when we are limited in the vectors that we can create? What happens when two vectors exist in the same direction, or exist on the same line? The combination of those vectors is also stuck on the same line, limiting our span to jsut that line. Now matter how you scale itm, the resulting sum vector is also stuck on the same line. \n",
    "\n",
    "This makes them *Linearly Dependent*. In a three dimensional space, when we have a lienarly depdent set of vectors, we often get stuck on a plane in a smaller numer of dimensions. \n",
    "\n",
    "Why do we care whether two vectors are linearly dependent or independent? A lost of problesm become difficult or unsolveable when they are linearly depdendent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Transformations\n",
    "\n",
    "This concept of adding two vectors with fixed direction, but scaling them to get different combined vectors is hugely important. This combined vector, except in cases of linear dependence, can point in any direction.  and have any length we choose. This sets up an intuition for linear transformations where we use a vector to transform another vector in a function-like manner. \n",
    "\n",
    "### Basis Vectors\n",
    "\n",
    "Imagine we have two simple vectors $\\widehat{i}$ and $\\hat{j}$. These are known as *basis vectors*, which are used to describe transformations on other vectors. They typically have a length of 1 and point in perpendicular positive directions.\n",
    "\n",
    "Think of basis vectors as building blocks to build or transform any vector. Our basis vector is expressed in a 2 x 2 matrix, where the first column is $\\hat{i}$ and the second column is $\\hat{j}$. \n",
    "\n",
    "$\\hat{i} \\quad = \\quad \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "$\\hat{j} \\quad = \\quad \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "$\\text{basis} \\quad = \\quad \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "a matrix is a collection of vectors that can have multiple rows and coloumns and is a convenient way of packaging data. We can use $\\hat{i}$ and $\\hat{j}$ to create any vector we want by scaling and adding them.\n",
    "\n",
    "I want $\\vec{v}$ to land at [3,2]. What happens to $\\vec{v}$ if we stretch $\\hat{i}$ by a factor of 3 and $\\hat{j}$ by a factor of 2? First we scale individually:\n",
    "\n",
    "$3\\hat{i} \\quad = \\quad 3\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\quad = \\quad \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "$2\\hat{j} \\quad = \\quad 2\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\quad = \\quad \\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix}$\n",
    "\n",
    "If we stretched space in these two directions, what does this do to $\\vec{v}$? It will stretch with $\\hat{i}$ and $\\hat{j}$. This is known as a *linear transformation*, where we transform a vector with stretching, squishing, sheering, or rotating by tracking basis vector movements. \n",
    "\n",
    "Recall that $\\vec{v}$ is composed of adding $\\hat{i}$ and $\\hat{j}$, so we simply take the stretched $\\hat{i}$ and $\\hat{j}$ and add them together to see where vector $\\vec{v}$ has landed:\n",
    "\n",
    "$\\vec{v}_{new} \\quad = \\quad \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix} \\quad = \\quad \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}$\n",
    "\n",
    "Generally, with linear transformations there are four movements you can achieve: *Scale, Rotate, Shear, Inversion*\n",
    "\n",
    "- Scaling a vector will stretch or squeeze it\n",
    "- Rotations will turn the vector space\n",
    "- Inversions will flip the vector space so that $\\hat{i}$ and $\\hat{j}$ swap respective places. \n",
    "- A shear displaces each point in a fixed direction proportionally to its distance to a given line parallel to that direction.\n",
    "\n",
    "You cannot have transformations that are nonlinear, resulting in curvy or squiggly transformations that are no longer in a straight line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Vector Multiplication\n",
    "\n",
    "This concept of tracking where $\\hat{i}$ and $\\hat{j}$ land after a transformation is important because it allows us not just to create vectors, but also to transform existing vectors.\n",
    "\n",
    "The formula to transform a vector $\\vec{v}$ given basis vectors $\\hat{i}$ and $\\hat{j}$ packaged as a matrix is:\n",
    "\n",
    "$\\begin{bmatrix} x_{new} \\\\ y_{new} \\end{bmatrix} \\quad = \\quad \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} x_{new} \\\\ y_{new} \\end{bmatrix} \\quad = \\quad \\begin{bmatrix} ax + by \\\\ cx + dy \\end{bmatrix}$\n",
    "\n",
    "$\\hat{i}$ is the first column $[a , c]$ and $\\hat{j}$ is the column $[b, d]$. We package both of these basis vectors as a matrix; which, again, is a collection of vectors expressed as a grid of snumbers in two ore more directions. This transformation of a vector by applying basis vectors is known as *matrix vector multiplication* This formula is a shortcut fo scaling and adding $\\hat{i}$ and $\\hat{j}$ just like we did earlier adding two vectors, and applying the transformation to any vector $\\vec{v}$. \n",
    "\n",
    "In effect, a matrix really is a transformation expressed as basis vectors. \n",
    "\n",
    "To execute this in numpy we need to declare our basis vectors as a matrix and then apply it to vector $\\vec{v}$ using the `dot()` operator. The `dot()` operator will perform this caling and addition between our matrix and vector. This is known as the *dot product*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2]\n"
     ]
    }
   ],
   "source": [
    "# compose basis matrix with i-hat and j-hat\n",
    "basis = np.array([[3, 0], [0, 2]])\n",
    "\n",
    "#declare vector v\n",
    "v = np.array([1, 1])\n",
    "\n",
    "#create new vector by transforming v with dot product\n",
    "new_v = basis.dot(v)\n",
    "\n",
    "print(new_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When thinking in terms of basis vectors, I prefer to break out the basis vectors and then compose them together into a matrix. Just note you will need to *traanspose*, or swap the columns and rows. This is because NumPy's `array()` function will do the opposite orientation we want. Populating each vector as a row rather than a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3]\n"
     ]
    }
   ],
   "source": [
    "# transposition\n",
    "\n",
    "#declare i-hat and j-hat\n",
    "i_hat = np.array([2, 0])\n",
    "j_hat = np.array([0, 3])\n",
    "\n",
    "#compose basis matrix using i-hat and j-hat, also need to transpose into columns\n",
    "basis = np.array([i_hat, j_hat]).transpose()\n",
    "\n",
    "#declare vector v\n",
    "v = array([1, 1])\n",
    "\n",
    "#create new vector by transforming v with dot product\n",
    "new_v = basis.dot(v)\n",
    "\n",
    "print(new_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example. Let's start with vector $\\vec{v}$ being [2,1] and $\\hat{i}$ and $\\hat{j}$ start at [1, 0] and [0, 1], respectively. We then transform $\\hat{i}$ and $\\hat{j} to [2, 0] and [0, 3]. \n",
    "\n",
    "What happens to vector $\\vec{v}$? Working this out mathematically by hand using our formula, we get this:\n",
    "\n",
    "$\\begin{bmatrix} x_{new} \\\\ y_{new} \\end{bmatrix} \\quad = \\quad \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} x_{new} \\\\ y_{new} \\end{bmatrix} \\quad = \\quad \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\quad = \\quad \\begin{bmatrix} (2)(2) + (0)(1) \\\\ (2)(0) + (3)(1) \\end{bmatrix} \\quad = \\quad \\begin{bmatrix} 4 \\\\ 3 \\end{bmatrix}$\n",
    "\n",
    "This solution using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3]\n"
     ]
    }
   ],
   "source": [
    "#declare i-hat and j-hat\n",
    "i_hat = np.array([2, 0])\n",
    "j_hat = np.array([0, 3])\n",
    "\n",
    "#compose basis matrix using i-hat and j-hat, also need to transpose into columns\n",
    "basis = np.array([i_hat, j_hat]).transpose()\n",
    "\n",
    "# declare vector v 0\n",
    "v = np.array([2, 1])\n",
    "\n",
    "# create new vector by transforming v with dot product\n",
    "new_v = basis.dot(v)\n",
    "\n",
    "print(new_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector $\\vec{v}$ now lands at [4,3]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "We learned how to multiply a vector and amatrix, but what exactly does multiplying two matrices accomplish? Think of *matrix multiplication* as applying multiple transformations to a vector space. Each transformation is like a function, where we apply the innermost first and then apply each subsequent transformation outward.\n",
    "\n",
    "here is how we apply a rotation and then a shear to any vector $\\vec{v}$ with value $[v, y]$:\n",
    "\n",
    "$\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix}$\n",
    "\n",
    "We can actually consolidate these two transformations by using this formula. Applying one transformation onto the last. You multiply and add each row from the first matrix to each respective column of the second matrix, in an \"over-and-down\"! \"over-and-down\"! pattern:\n",
    "\n",
    "$\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} e & f \\\\ g & h \\end{bmatrix} \\quad = \\quad \\begin{bmatrix} ae + bg & af + bh \\\\ ce + dg & cf + dh \\end{bmatrix}$\n",
    "\n",
    "So we can actually consolidate these two separate transformations (rotation and shear) into a single transformation:\n",
    "\n",
    "$\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix}$\n",
    "\n",
    "$= \\quad \\begin{bmatrix} (1)(0) + (1)(1) & (-1)(1) + (1)(0) \\\\ (0)(0) + (1)(1) & (0)(-1) + (1)(0) + (1) \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix}$\n",
    "\n",
    "$= \\quad \\begin{bmatrix} 1 & -1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix}$\n",
    "\n",
    "To execute this in python using numpy, you can combine the two matrices simply using the `matmul()` or `@` opearator. We will then turn around and use this consolidated transformation and apply it to a vector [1,2].  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED MATRIX:\n",
      " [[ 1 -1]\n",
      " [ 1  0]]\n",
      "[-1  1]\n"
     ]
    }
   ],
   "source": [
    "#transformation 1\n",
    "i_hat1 = np.array([0, 1])\n",
    "j_hat1 = np.array([-1, 0])\n",
    "transform1 = array([i_hat1, j_hat1]).transpose()\n",
    "\n",
    "#transformation 2\n",
    "i_hat2 = np.array([1, 0])\n",
    "j_hat2 = np.array([1, 1])\n",
    "transform2 = array([i_hat2, j_hat2]).transpose()\n",
    "\n",
    "#combine transformations\n",
    "combined = transform2 @ transform1\n",
    "\n",
    "#test\n",
    "print('COMBINED MATRIX:\\n {}'.format(combined))\n",
    "\n",
    "v = np.array([1, 2])\n",
    "print(combined.dot(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the order of the transformation matters!! If we apply `transformation1` on `transformation2`, we get a different result of [-2, 3]. So matrix dot products are not commutative, meaning you cannot flip the order and expect the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinants\n",
    "When we perform linear transformations, we sometimes \"expand\" or \"squish\" space and the degreee this happens can be helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.000000000000001\n"
     ]
    }
   ],
   "source": [
    "# a negative determinant\n",
    "from numpy.linalg import det\n",
    "\n",
    "i_hat = np.array([-2, 1])\n",
    "j_hat = np.array([1, 2])\n",
    "\n",
    "basis = array([i_hat, j_hat]).transpose()\n",
    "\n",
    "determinant = det(basis)\n",
    "\n",
    "print(determinant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this determinant is negative, we suiqkcly see that the orientation has flipped. But by far the most critical piece of information the determinatn tells you is whether the transformation is linearly dependent. \n",
    "\n",
    "> If you have a determinant of 0, that means all of the space ahs been squished into a lesser dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# a determinant of 0\n",
    "i_hat = np.array([3, -1.5])\n",
    "j_hat = np.array([-2, 1])\n",
    "basis = array([i_hat, j_hat]).transpose()\n",
    "determinant = det(basis)\n",
    "print(determinant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, testing for a 0 determinant is highly helpful to determine if a transformation has linear dependence. When you encounter this you will likley find a difficult or unsolvable problem on your hands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Types of Matrices\n",
    "There are a few notable cases of matrices that we should cover.\n",
    "\n",
    "### Square Matrix \n",
    "The *square matrix* is a matrix that has an equal number of rows and columns:\n",
    "\n",
    "$\\begin{bmatrix} 4 & 2 & 7 \\\\ 5 & 1 & 9 \\\\ 4 & 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "They are primarily used to represent linear transformations and are a requriement of many operations like eigendecomposition. \n",
    "\n",
    "### Identity Matrix\n",
    "The *identity matrix* is a quare matrix that has a diagonal of 1s while the other values are 0:\n",
    "\n",
    "$\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "Whats the big deal with identity matrices? Well, when you have an idneity matrix, you essentially have undone a transformation and now found your starting basis vectors. This will play a big role in solving systems of equations.\n",
    "\n",
    "### Inverse Matrix\n",
    "An *inverse matrix* is a matrix that undoes the transformation of another matrix Let's say I have matrix A:\n",
    "\n",
    "$\\begin{bmatrix} 4 & 2 & 4 \\\\ 5 & 3 & 7 \\\\ 9 & 3 & 6 \\end{bmatrix}$\n",
    "\n",
    "The inverse of matrix $A$ is called $A^{-1}$. The inverse of matrix $A$ is:\n",
    "\n",
    "$\\begin{bmatrix} -\\frac{1}{2} & 0 & \\frac{1}{3} \\\\ 5.5 & -2 & -\\frac{4}{3} \\\\ -2 & 1 & \\frac{1}{3} \\end{bmatrix}$\n",
    "\n",
    "When we perform matrix multiplication between $A^{-1}$ and $A$, we end up with an identity matrix.\n",
    "\n",
    "$\\begin{bmatrix} -\\frac{1}{2} & 0 & \\frac{1}{3} \\\\ 5.5 & -2 & -\\frac{4}{3} \\\\ -2 & 1 & \\frac{1}{3} \\end{bmatrix} \\begin{bmatrix} 4 & 2 & 4 \\\\ 5 & 3 & 7 \\\\ 9 & 3 & 6 \\end{bmatrix} \\quad = \\quad \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "### Diagonal Matrix \n",
    "Similar to the idneity matrix is the *diagonal matrix*, which has a diagonal of nonzero values while the rest of the values are 0. Diagonal matrices are desirable in certain computtion because they represent simple sclaras being applied to a vector space. It shows up in some linear algebra operations\n",
    "\n",
    "$\\begin{bmatrix} 4 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 5 \\end{bmatrix}$\n",
    "\n",
    "### Triangular Matrix \n",
    "\n",
    "Similar to the diagonal matrix is the *triangular matrix, which has a diagonal of nonzero values in fromt of a triangle of values, while the rest of the values are 0.\n",
    "\n",
    "$\\begin{bmatrix} 4 & 2 & 9 \\\\ 0 & 1 & 6 \\\\ 0 & 0 & 5 \\end{bmatrix}$\n",
    "\n",
    "Triangular matrices are desirable in many numerical anlysis tasks, because they are typically easier to solve in systems of equations. They also show up in certain decomposition tasks like LU Decomposition.\n",
    "\n",
    "### Sparse Matrix\n",
    "Occasionally, you will run into matrices that are mostly zeroes and have very few non-zero elements. These are called *sparse matrices*. From a computing standpoint, they create opportunities to create efficiencies. If a matrix has mostly 0s, a sparse matrix implementation will not wasted space storing a bunch of 0s, and instead only track the cells that are non-zero.\n",
    "\n",
    "$\\text{sparse} \\quad = \\quad \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 2 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$\n",
    "\n",
    "When you have large matrices that are sparse, you might explicitly use a sparse function to create your matrix.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systems of Equations and Inverse Matrices\n",
    "\n",
    "One of the basic use cases for linear algebra is solving systems of linear equations. It is also a good application to learn about inverse matrices. Let's say you are provided with the following equations and need to solve for $x$, $y$, and $z$:\n",
    "\n",
    "$4x + 2y + 4z \\quad = \\quad 44$\n",
    "\n",
    "$5x + 3y + 7z \\quad = \\quad 56$\n",
    "\n",
    "$9x + 3y + 6z \\quad = \\quad 72$\n",
    "\n",
    "You can try manually experimenting with different algebraic operations to isolate the three variables, but if you want a computre to solve it you will need to express the problem in terms of matrices shown next:\n",
    "- Extract the coefficients into matrix $A$\n",
    "- The values on the right side into matrix $B$\n",
    "- The unknown variables into matrix $X$\n",
    "\n",
    "$A \\quad = \\quad \\begin{bmatrix} 4 & 2 & 4 \\\\ 5 & 3 & 7 \\\\ 9 & 3 & 6 \\end{bmatrix}$\n",
    "\n",
    "$B \\quad = \\quad \\begin{bmatrix} 44 \\\\ 56 \\\\ 72 \\end{bmatrix}$\n",
    "\n",
    "$X \\quad = \\quad \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}$\n",
    "\n",
    "The function for a linear system of equations is $AX = B$. We need to tranform matrix $A$ with some other matrix $X$ that will result in matrix $B$.\n",
    "\n",
    "$AX \\quad = \\quad B$\n",
    "\n",
    "$\\begin{bmatrix} 4 & 2 & 4 \\\\ 5 & 3 & 7 \\\\ 9 & 3 & 6 \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\quad = \\quad \\begin{bmatrix} 44 \\\\ 56 \\\\ 72 \\end{bmatrix}$\n",
    "\n",
    "We need to \"undo\" $A$ so that we can isolate $X$ and get the values for $x$, $y$, and $z$. The way you undo $A$ is to take the inverse of $A$, (denoted $A^{-1}$) and apply it to $A$ via matrix multiplication. We can express this algebraically:\n",
    "\n",
    "$AX \\quad = \\quad B$\n",
    "\n",
    "$A^{-1}AX \\quad = \\quad A^{-1}B$\n",
    "\n",
    "$X \\quad = \\quad A^{-1}B$\n",
    "\n",
    "To calculate the inverse of $A$, we probably use a computre rather than searching for solutions using gaussian elimination Here is the inverse of matrix $A$:\n",
    "\n",
    "$\\begin{bmatrix} -\\frac{1}{2} & 0 & \\frac{1}{3} \\\\ 5.5 & -2 & -\\frac{4}{3} \\\\ -2 & 1 & \\frac{1}{3} \\end{bmatrix}$\n",
    "\n",
    "Note when we matrix multiply $A^{-1}$ against $A$  it will create an identity matrix, a matrix of all zeroes except for 1s in the diagonal. The idnetity matrix is the linear algebra equivalent of multiplying by 1,, meaning it essentially has no effect and will effectively isolate values for $x$, $y$, and $z$:\n",
    "\n",
    "$A^{-1} \\quad = \\quad \\begin{bmatrix} -\\frac{1}{2} & 0 & \\frac{1}{3} \\\\ 5.5 & -2 & -\\frac{4}{3} \\\\ -2 & 1 & \\frac{1}{3} \\end{bmatrix}$\n",
    "\n",
    "$A \\quad = \\quad \\begin{bmatrix} 4 & 2 & 4 \\\\ 5 & 3 & 7 \\\\ 9 & 3 & 6 \\end{bmatrix}$\n",
    "\n",
    "$A^{-1}A \\quad = \\quad \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "To see this identity matrix in action using python, you will want to use sympy instead of numpy. The floating point decimals in NumPy will not make the identiy matrix as obvious, but doing it symbolically we will see a clean, symbolic output. Note that to do matrix multiplication in SymPy we use the asterisk `*` rather than `@`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INVERSE: Matrix([[-1/2, 0, 1/3], [11/2, -2, -4/3], [-2, 1, 1/3]])\n"
     ]
    }
   ],
   "source": [
    "from sympy import *\n",
    "\n",
    "# 4x + 2y + 4z = 44\n",
    "# 5x + 3y + 7z = 56\n",
    "# 9x + 3y + 6z = 72\n",
    "\n",
    "A = Matrix([[4, 2, 4], [5, 3, 7], [9, 3, 6]])\n",
    "\n",
    "#dot product between A and its inverse, will produce identity function\n",
    "inverse = A.inv()\n",
    "identity = inverse * A\n",
    "\n",
    "print(\"INVERSE: {}\".format(inverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDENTITY: Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(\"IDENTITY: {}\".format(identity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same calculation in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2. 34. -8.]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# 4x + 2y + 4z = 44\n",
    "# 5x + 3y + 7z = 56\n",
    "# 9x + 3y + 6z = 72\n",
    "\n",
    "A = np.array([[4, 2, 4], [5, 3, 7], [9, 3, 6]])\n",
    "B = np.array([44, 56, 72])\n",
    "X = inv(A).dot(B)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $x = 2$, $y = 34$, and $z = -8$\n",
    "\n",
    "The next example shows the full solution in SymPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix([[2], [34], [-8]])\n"
     ]
    }
   ],
   "source": [
    "from sympy import *\n",
    "\n",
    "# 4x + 2y + 4z = 44\n",
    "# 5x + 3y + 7z = 56\n",
    "# 9x + 3y + 6z = 72\n",
    "\n",
    "A = Matrix([[4, 2, 4], [5, 3, 7], [9, 3, 6]])\n",
    "B = Matrix([44, 56, 72])\n",
    "X = A.inv() * B\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practicality, you should rarely find it necessary to calculate inverse matrices by hand and can have a computer do it for you. But if you have a need or are curious you need to learn about Gaussian Elimination. PatrickJMT on YOutube has a numbewr of videso demonstrating Gaussian Elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectors and Eigenvalues\n",
    "\n",
    "*Matrix Decomposition* is breaking up a matrix into its basic components, much like factoring numbers (e.g., 10 and be factored to 2 x 5). Matrix decomposition is helpful for tasks like finding inverse matrices and calculating determinants, as well as linear regression. There are many ways to decompose a matrix depending on your task. \n",
    "\n",
    "A common method is called *Eigendecomposition*, which is often used for machine learning and principal component analysis. Eigendocmposition is helpful for breaking up a matrix into components that are easier to work with in differnt machine learnign tasks. It also only works on square matrices. In eigendecomposition there are two components:\n",
    "- The eigenvalues denoted by lambda $\\lambda$\n",
    "- eigenvector $v$\n",
    "\n",
    "If we have a square matrix $A$, it has the following eigenvalue equation:\n",
    "\n",
    "$Av \\quad = \\quad \\lambda v$\n",
    "\n",
    "If $A$ is the original matrix, it is composed of eigenvector $v$ and eigenvalue $\\lambda$. There is one eigenvector and eigenvalue for each dimension of the parent matrix, and not all matrices can be decomposed into an eigenvector and eigenvalue. Somtimes complex (imaginary) values will even result. \n",
    "\n",
    "This example is how we calculate eigenvectors and eigenvalues in NumPy for a given matrix $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing eigendecomposition in NumPy\n",
    "from numpy import array, diag\n",
    "from numpy.linalg import eig, inv\n",
    "\n",
    "A = array([[1, 2],[3, 4]])\n",
    "eigenvals, eigenvecs = eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EIGENVALUES\n",
      "[-0.37228132  5.37228132]\n"
     ]
    }
   ],
   "source": [
    "print(\"EIGENVALUES\")\n",
    "print(eigenvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EIGENVECTORS\n",
      "[[-0.82456484 -0.41597356]\n",
      " [ 0.56576746 -0.90937671]]\n"
     ]
    }
   ],
   "source": [
    "print(\"EIGENVECTORS\")\n",
    "print(eigenvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we rebuild matrix $A$ from the eigenvectors and eigenvalues?\n",
    "\n",
    "Recall the formula: \n",
    "\n",
    "$Av \\quad = \\quad \\lambda v$\n",
    "\n",
    "We need to make a few tweaks to the formula to reconstruct $A$:\n",
    "\n",
    "$A \\quad = \\quad Q\\Lambda A^{-1}$\n",
    "\n",
    "In this new formula, $Q$ is the eigenvectors, $\\Lambda$ is the eigenvalues in diagonal form, and $Q^{-1}$ is the inverse matrix of $Q$. Diagonal form means the vector is padded into a matrix of zeroes and occupies the diagonal line in a similar pattern to an identity matrix. \n",
    "\n",
    "The below example brings this concept full circle in Python, starting with decomposing the matrix, and then recomposing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EIGENVALUES\n",
      "[-0.46410162  6.46410162]\n"
     ]
    }
   ],
   "source": [
    "# Decomposing and recomposing a matrix in NumPy\n",
    "from numpy import array, diag\n",
    "from numpy.linalg import eig, inv\n",
    "\n",
    "A = array([[1, 2], [4, 5]])\n",
    "eigenvals, eigenvecs = eig(A)\n",
    "\n",
    "print(\"EIGENVALUES\")\n",
    "print(eigenvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EIGENVECTORS\n",
      "[[-0.80689822 -0.34372377]\n",
      " [ 0.59069049 -0.9390708 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"EIGENVECTORS\")\n",
    "print(eigenvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REBUILD MATRIX\n",
      "[[1. 2.]\n",
      " [4. 5.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"REBUILD MATRIX\")\n",
    "Q = eigenvecs\n",
    "R = inv(Q)\n",
    "\n",
    "L = diag(eigenvals)\n",
    "B = Q @ L @ R\n",
    "print(B) #prints the matrix we started with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
