{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading | James, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2023). An Introduction to Statistical Learning: With Applications in Python. Springer. \n",
    "\n",
    "Read up to the end of Section 2.1.1. In this chapter, a general introduction to the notation and basic concepts is presented; we didnâ€™t cover parametric vs non-parametric, but it is worth knowing about. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical learning refers to a vast set of tools for understanding data. These tools can be classifed as supervised or unsupervised. Broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs. Problems of this nature occur in felds as diverse as business, medicine, astrophysics, and\n",
    "public policy. With unsupervised statistical learning, there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data.\n",
    "\n",
    "### ISL is based on the following four premises:\n",
    "1. Many statistical learning methods are relevant and useful in a wide range of academic and non-academic disciplines, beyond just the statistical sciences.\n",
    "2. Statistical learning should not be viewed as a series of black boxes.\n",
    "3. While it is important to know what job is performed by each cog, it is not necessary to have the skills to construct the machine inside the box!\n",
    "4. We presume that the reader is interested in applying statistical learning methods to real-world problems.\n",
    "\n",
    "### Notation and Simple Matrix Algebra\n",
    "- $n$ to represent the number of distinct data points or observations.\n",
    "- $p$ to denote the number of variables that are available for use in making predictions.\n",
    "- $x_{ij}$ represents the value of the $j$th variable for the $i$th observation, where $i = 1,2, \\ldots, n$ and $j = 1,2, \\ldots, p$.\n",
    "- We let $X$ denote an $n \\times p$ matrix whose *ij*th element is $x_{ij}$. That is,\n",
    "\n",
    "$$ X \\quad = \\quad \\begin{pmatrix} x_11 & x_12 & \\ldots & x_{ip} \\\\ x_21 & x_22 & \\ldots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & \\ldots & x_{np} \\end{pmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Why estimate $f$?\n",
    "\n",
    "There are two main reasons that we may wish to estimate $f$: prediction and inference. We discuss each in turn.\n",
    "\n",
    "### Prediction\n",
    "In many situations, a set of inputs X are readily available, but the output Y cannot be easily obtained. In this setting, since the error term averages to zero, we can predict Y using\n",
    "$$\\hat{Y} = \\hat{f}(X)$$\n",
    "\n",
    "The accuracy of $\\hat{Y}$ as a prediction for $Y$ depends on two quantities, which we will call the *reducible error* and the *irreducible error*. In general, $\\hat{f}$ will not be a perfect estimate for $f$, and this inaccuracy will introduce some error. This error is *reducible* because we can potentially improve the accuracy of $\\hat{f}$ by using the most appropriate statistical learning technique to estimate $f$.\n",
    "\n",
    "However, even if it were possible to form a perfect estimate for $f$, so that our estimated response took the form $\\hat{Y} = f(X)$, our prediction would still have some error in it. This is because $Y$ is also a function of $\\epsilon$, which, by definition cannot be predicted using $X$.\n",
    "- Therefore, variablility associated with $\\epsilon$ also affects the accuracy of our predictions. This is known as the irreducible error, because no matter how well we estimate $f$, we cannot reduce the error introduced by $\\epsilon$. \n",
    "\n",
    "Why is the irreducible error larger than zero? The quantity $\\epsilon$ may contain unmeaasured variables that are useful in predicting $Y$: since we don't measure them $f$ cannot use them for prediction. The quantity $\\epsilon$ may also contain unmeasured variation. \n",
    "\n",
    "Consider a given estimate $\\hat{f}$ and a set of predictors $X$, which yields the prediction $\\hat{Y} = \\hat{f}(X)$. Assume for a moment that both $\\hat{f}$ and $X$ are fixed, so that the only variability comes from $\\epsilon$. Then, it is easy to show that \n",
    "\n",
    "\n",
    "\n",
    "$E(Y - \\hat{Y})^2 \\quad = \\quad E[f(X) + \\epsilon - \\hat{f}(X)]^2$\n",
    "\n",
    "$\\quad \\quad = \\quad  \\underbrace{[f(X) - \\hat{f}(X)]^2}_{\\text{Reducible}} + \\underbrace{\\text{Var}(\\epsilon)}_{\\text{Irreducible}}$\n",
    "\n",
    "Where $E(Y - \\hat{Y})^2$ represents the average, or *expected value* of the suqred difference between the predicted and actual value of $Y$, and $\\text{Var} (\\epsilon)$ represents the *variance* associated with the error term $\\epsilon$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
