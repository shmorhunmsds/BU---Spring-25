{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "A Complete Guide to Matrix Notation and Linear Regression\n",
        "=========================================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s really understand matrix notation in context of linear regression,\n",
        "from the ground up.\n",
        "\n",
        "Linear Regression finds the best line, or *hyperplane* $\\hat{y}$\n",
        "in higher dimension, or generally a function $f$:\n",
        "\n",
        "\\begin{align}\\hat{y} = f(x) = wx\\end{align}\n",
        "\n",
        "that fits the whole data. This is just a dot product between vector\n",
        "$w$ and a data point $x$ in $d$ dimension:\n",
        "\n",
        "\\begin{align}\\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_dx_d\\end{align}\n",
        "\n",
        "Notice that we use $w_0$ as an intercept term, and thus we need to\n",
        "add a dummy dimension with value of “1” ($x_0$) for all data\n",
        "points $x$. Thus, $x$ here is on $d+1$ dimension.\n",
        "Think of it as the y-intercept term $c$ in 2-dimension\n",
        "($y = mx + c$).\n",
        "\n",
        "Another way to look at this is that $f(x)$ transforms a data point\n",
        "$x$ on $d+1$ dimension into a predicted scalar value\n",
        "$\\hat{y}$ that is close to target $y$:\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "   x_0 \\\\\n",
        "   x_1 \\\\\n",
        "   \\vdots \\\\\n",
        "   x_d \n",
        "   \\end{bmatrix}\n",
        "   \\xrightarrow{f}\n",
        "   \\hat{y}\n",
        "   \\approx y\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Sum of Squared Error Loss\n",
        "-----------------------------\n",
        "\n",
        "The best way to solve this is to find $w$ that minimizes the **sum\n",
        "of squared errors (SSE)**, or the “error” between all of predicted value\n",
        "$\\hat{y}^i$ and the target $y^i$ of $i^{th}$ data\n",
        "point for $i = 1$ to $n$, writing this as a loss function\n",
        "$L(w)$:\n",
        "\n",
        "\\begin{align}L(w) = \\sum_{i=1}^{n} \\left( y^i - \\hat{y}^i \\right)^2 = \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2\\end{align}\n",
        "\n",
        "From now on we refer to a data point (d+1 vector) as $x^i$ and its\n",
        "corresponding target (scalar) as $y^i$. I know it’s confusing for\n",
        "the first time, but you’ll get used to using superscript for indexing\n",
        "data points.\n",
        "\n",
        "Surprisingly, the SSE loss is not from someone’s intuition, but it’s\n",
        "from the assumption that there is **Gaussian noise in our observation**\n",
        "of the underlying linear relationship. We will show how this leads to\n",
        "SSE loss later, but first let’s visualize what we’re trying to do.\n",
        "\n",
        ".. raw:: html\n",
        "\n",
        "   <center>\n",
        "\n",
        "| |linear regression|\n",
        "\n",
        ".. raw:: html\n",
        "\n",
        "   </center>\n",
        "\n",
        "1. There is a true line, the true linear relationship that we want to\n",
        "   discover (blue line).\n",
        "2. The data points are then observed through noise **deviating from that\n",
        "   line, with Gaussian distribution**.\n",
        "3. Suppose we predict a random line (red), not the best one yet.\n",
        "4. We calculate the distance or difference between the predicted points\n",
        "   (along the line) and the actual data points. This is then **squared\n",
        "   and sum up to get sum of squared error**.\n",
        "\n",
        "..\n",
        "\n",
        "   Linear regression is the method to get the line that fits the given\n",
        "   data with the minimum sum of squared error.\n",
        "\n",
        ".. |linear regression| image:: imgs/img_lr_objective.png\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How to Find the Optimal Solution\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "An optimal solution ($w$) for this equation can be found either\n",
        "using *closed-form solution* or via iterative methods like gradient\n",
        "descent.\n",
        "\n",
        "**A closed-form solution** means we figure out the formula for\n",
        "$w = ...$. Implementing that formula in a program directly solves\n",
        "the problem. The thing is you have to come up with the correct one\n",
        "yourself, by hand.\n",
        "\n",
        "Do you remember how to find a minimum (or maximum) value for a function?\n",
        "We take the derivative of the function above with respect to $w$,\n",
        "set it to zero, and solve for the $w$ in terms of other\n",
        "parameters. This is like taking a single jump to the optimal value. We\n",
        "do all the hard work for computers.\n",
        "\n",
        "Luckily we can do this for linear regression, but not all loss functions\n",
        "be solved this way, actually, only a few. In those cases, we use\n",
        "**iterative methods like gradient descent** to search for the solution.\n",
        "In contrast to closed-form solution, we do not jump directly to the\n",
        "optimal answer, instead, we take many steps that lead us near to where\n",
        "the optimal answer lives.\n",
        "\n",
        "Next let’s derive the closed-form solution for linear regression. In\n",
        "order to do that efficiently, we need some matrix notations.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Going into Matrix Notation\n",
        "--------------------------\n",
        "\n",
        "Writing things down in matrix notation makes things much faster in\n",
        "NumPy. **But it’s not easy to read matrix notation, especially if you\n",
        "study machine learning on your own.** There’re things like dot product,\n",
        "matrix multiplication, transpose and stuff that you need to keep track\n",
        "of in your head. If you’re starting out, then please write them on\n",
        "papers, drawing figures as needed to make you understand. It really pays\n",
        "off.\n",
        "\n",
        "On top of that, these few **key standards** will make our lives with\n",
        "linear algebra easier:\n",
        "\n",
        "1. Always a *column* vector\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "When you see standalone vectors in a matrix notation formula, assumes\n",
        "it’s a column vector. E.g.,\n",
        "\n",
        "\\begin{align}x = \n",
        "   \\begin{bmatrix}\n",
        "   1 \\\\\n",
        "   2 \\\\\n",
        "   3 \\\\\n",
        "   \\end{bmatrix}\\end{align}\n",
        "\n",
        "and so its transpose is a row vector,\n",
        "\n",
        "\\begin{align}x^T = \n",
        "   \\begin{bmatrix}\n",
        "   1 & 2 & 3\n",
        "   \\end{bmatrix}\\end{align}\n",
        "\n",
        "Likewise, you should try to make the final result of matrix operation to\n",
        "be a column vector.\n",
        "\n",
        "Note that the NumPy vector created by ``np.zeros``, ``np.arange``, etc.,\n",
        "is not really a column vector. It has only one dimension ``(N,)``. So,\n",
        "you cannot transpose it directly (``x.T`` still gives you ``x``.) To\n",
        "convert it to a column vector, we use ``x.reshape(N,1)`` or\n",
        "``x[:, None]``.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Feature matrix $X$ is rows of data points\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "Our data points $x^i$ are on $d+1$ dimension, and there is\n",
        "$n$ of them, we store them all in a 2-d matrix $X$:\n",
        "\n",
        "\\begin{align}X = \\begin{align}\n",
        "   \\underset{n\\times d}\n",
        "   {\\begin{bmatrix}\n",
        "   \\longleftarrow & x^1 & \\longrightarrow \\\\\n",
        "   \\longleftarrow & x^2 & \\longrightarrow \\\\\n",
        "   & \\vdots & \\\\\n",
        "   \\longleftarrow & x^n & \\longrightarrow \\\\\n",
        "   \\end{bmatrix}}\n",
        "   \\end{align}\\end{align}\n",
        "\n",
        "Each row in $X$ is a row vector for each data point. Also note\n",
        "that we use uppercase letter for matrix.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Again, $w$ is a column vector\n",
        "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "Like the first point, our $w$ will be $d+1$ dimension column\n",
        "vector with w_0 as an intercept term:\n",
        "\n",
        "\\begin{align}w = \n",
        "   \\begin{bmatrix}\n",
        "   w_0 \\\\\n",
        "   w_1 \\\\\n",
        "   \\vdots \\\\\n",
        "   w_d \n",
        "   \\end{bmatrix}\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Dot products of rows in matrix $X$ with vector $w$ is $Xw$\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "Sometimes we want the dot product of each row in matrix with a vector:\n",
        "\n",
        "\\begin{align}Xw = \\underset{n\\times (d+1)}\n",
        "   {\n",
        "       \\begin{bmatrix}\n",
        "       \\longleftarrow & x^1 & \\longrightarrow \\\\\n",
        "       \\longleftarrow & x^2 & \\longrightarrow \\\\\n",
        "       & \\vdots & \\\\\n",
        "       \\longleftarrow & x^n & \\longrightarrow \\\\\n",
        "       \\end{bmatrix}\n",
        "   }\n",
        "   \\underset{(d+1) \\times 1}\n",
        "   {\n",
        "       \\begin{bmatrix}\n",
        "       \\uparrow \\\\\n",
        "       w \\\\\n",
        "       \\downarrow\n",
        "       \\end{bmatrix}\n",
        "   }\n",
        "   =\n",
        "   \\begin{bmatrix}\n",
        "   x^1w \\\\\n",
        "   x^2w \\\\\n",
        "   \\vdots \\\\\n",
        "   x^nw\n",
        "   \\end{bmatrix}\\end{align}\n",
        "\n",
        "given that $X$ contains rows of vectors we want to dot product\n",
        "with.\n",
        "\n",
        "Interestingly, this gives us a column vector of our predictions\n",
        "$\\hat{y}$:\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "   x^1w \\\\\n",
        "   x^2w \\\\\n",
        "   \\vdots \\\\\n",
        "   x^nw\n",
        "   \\end{bmatrix} =\n",
        "   \\begin{bmatrix}\n",
        "   \\hat{y}^1 \\\\\n",
        "   \\hat{y}^2 \\\\\n",
        "   \\vdots \\\\\n",
        "   \\hat{y}^n \\\\\n",
        "   \\end{bmatrix} = \\hat{y}\\end{align}\n",
        "\n",
        "It’s also good to remind yourself that it sums along dimension of\n",
        "$x^i$ and $w$:\n",
        "\n",
        "\\begin{align}Xw =\n",
        "   \\begin{bmatrix}\n",
        "   \\sum_{j=0}^{d} x_j^1w_j \\\\\n",
        "   \\sum_{j=0}^{d} x_j^2w_j \\\\\n",
        "   \\vdots \\\\\n",
        "   \\sum_{j=0}^{d} x_j^nw_j \\\\\n",
        "   \\end{bmatrix}\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Sum of Squared is $x^Tx$\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "This is a useful pattern to memorize. Sometimes we want the sum of\n",
        "squared of each element in arbitrary d-dimension vector $x$:\n",
        "\n",
        "\\begin{align}\\sum_{j=1}^{d} x_i^2\\end{align}\n",
        "\n",
        "which is simply $x^Tx$:\n",
        "\n",
        "\\begin{align}x^Tx = \n",
        "   \\begin{bmatrix}\n",
        "   x_1 & ... & x_d\n",
        "   \\end{bmatrix}\n",
        "   \\begin{bmatrix}\n",
        "   x_1 \\\\\n",
        "   \\vdots \\\\\n",
        "   x_d \n",
        "   \\end{bmatrix}\n",
        "   = \\sum_{j=1}^{d} x_i^2\\end{align}\n",
        "\n",
        "Notice that the result of $x^Tx$ is scalar, e.g., a number.\n",
        "\n",
        "In fancy term,\n",
        "${\\left\\lVert x \\right\\rVert} = \\sqrt{\\sum_{j=1}^{d} x_i^2}$ is\n",
        "L2-norm (or Euclidean norm) of $x$. So we can write sum of squared\n",
        "as ${\\left\\lVert x \\right\\rVert}^2 = \\sum_{j=1}^{d} x_i^2$. For\n",
        "now, let’s not care what norm actually means.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Writing SSE Loss in Matrix Notation\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "Now you’re ready, let’s write the above SSE loss function in matrix\n",
        "notation. If you look at $L(w)$ closely, it’s a sum of squared of\n",
        "vector $y - \\hat{y}$. This means we can kick-off by applying our\n",
        "fifth trick:\n",
        "\n",
        "\\begin{align}L(w) = {\\left\\lVert y - \\hat{y} \\right\\rVert}^2\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we just have to find $y - \\hat{y}$. First we encode target\n",
        "$y$ in a long column vector of shape ``[n, 1]``:\n",
        "\n",
        "\\begin{align}y = \n",
        "   \\begin{bmatrix}\n",
        "   y^1 \\\\\n",
        "   y^2 \\\\\n",
        "   \\vdots \\\\\n",
        "   y^n \n",
        "   \\end{bmatrix}\\end{align}\n",
        "\n",
        "(Remember that we use superscript for indexing the $i^{th}$ data\n",
        "point.)\n",
        "\n",
        "Next we encode each of our predicted values ($\\hat{y}^i$) in a\n",
        "column vector $\\hat{y}$. Since $\\hat{y}^i$ is a dot product\n",
        "between $w$ and each of $x^i$, we can apply\n",
        "`4 <#4.-Dot-products-of-rows-in-matrix-$X$-with-vector-$w$-is-$Xw$>`__:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\begin{align}\\begin{align}\n",
        "   y - \\hat{y} &= \n",
        "   \\begin{bmatrix}\n",
        "   y^1 \\\\\n",
        "   y^2 \\\\\n",
        "   \\vdots \\\\\n",
        "   y^n \n",
        "   \\end{bmatrix}\n",
        "   - \\begin{bmatrix}\n",
        "   \\hat{y}^1 \\\\\n",
        "   \\hat{y}^2 \\\\\n",
        "   \\vdots \\\\\n",
        "   \\hat{y}^n \n",
        "   \\end{bmatrix} && \\text{(Error between target and predicted)} \\\\ &=\n",
        "   \\begin{bmatrix}\n",
        "   y^1 \\\\\n",
        "   y^2 \\\\\n",
        "   \\vdots \\\\\n",
        "   y^n \n",
        "   \\end{bmatrix}\n",
        "   - \\begin{bmatrix}\n",
        "   x^1w \\\\\n",
        "   x^2w \\\\\n",
        "   \\vdots \\\\\n",
        "   x^nw\n",
        "   \\end{bmatrix} \n",
        "   && \\text{(Predicted is a dot product of $w$ and each of data point $x^i$)} \\\\ &=\n",
        "   \\underset{n\\times 1}\n",
        "   {\n",
        "       \\begin{bmatrix}\n",
        "       y^1 \\\\\n",
        "       y^2 \\\\\n",
        "       \\vdots \\\\\n",
        "       y^n \n",
        "       \\end{bmatrix}\n",
        "   }\n",
        "   - \\underset{n\\times (d+1)}\n",
        "   {\n",
        "       \\begin{bmatrix}\n",
        "       \\longleftarrow & x^1 & \\longrightarrow \\\\\n",
        "       \\longleftarrow & x^2 & \\longrightarrow \\\\\n",
        "       & \\vdots & \\\\\n",
        "       \\longleftarrow & x^n & \\longrightarrow \\\\\n",
        "       \\end{bmatrix}\n",
        "   }\n",
        "   \\underset{(d+1)\\times 1}\n",
        "   {\n",
        "       \\begin{bmatrix}\n",
        "       \\uparrow \\\\\n",
        "       w \\\\\n",
        "       \\downarrow\n",
        "       \\end{bmatrix}\n",
        "   } && \\text{(Separate them out)} \\\\ &=\n",
        "   y - Xw && \\text{(Encode in matrix/vector form)}\n",
        "   \\end{align}\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Putting all together we get our loss function for linear regression:\n",
        "\n",
        "\\begin{align}L(w) = {\\left\\lVert y - Xw \\right\\rVert}^2\\end{align}\n",
        "\n",
        "In NumPy code, we can compute $L(w) = (y - Xw)^T(y - Xw)$.\n",
        "\n",
        "There’s no intuitive way to come up with this nice formula the first\n",
        "time you saw it. You have to work it out and put things together\n",
        "yourself. Then you’ll start to memorize the pattern and it’ll become\n",
        "easier.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deriving a Closed-form Solution\n",
        "-------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To do that, we’ll take derivative of $L(w)$ with respect to\n",
        "$w$, set to zero and solve for $w$.\n",
        "\n",
        "Writing matrix notation is already hard, taking derivative of it is even\n",
        "harder. I recommend writing out partial derivatives to see what happens.\n",
        "For $L(w) = L_w$, we have to take derivative with respect to each\n",
        "dimension of $w$:\n",
        "\n",
        "\\begin{align}\\nabla L_w = \n",
        "   \\begin{bmatrix}\n",
        "   \\frac{\\partial L}{\\partial w_0} \\\\\n",
        "   \\frac{\\partial L}{\\partial w_1} \\\\\n",
        "   \\vdots \\\\\n",
        "   \\frac{\\partial L}{\\partial w_d} \\\\\n",
        "   \\end{bmatrix} \n",
        "   =\n",
        "   \\begin{bmatrix}\n",
        "   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_0} \\\\\n",
        "   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_1} \\\\\n",
        "   \\vdots \\\\\n",
        "   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_d}\n",
        "   \\end{bmatrix} \n",
        "   =\n",
        "   \\begin{bmatrix}\n",
        "   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_0} \\\\\n",
        "   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_1} \\\\\n",
        "   \\vdots \\\\\n",
        "   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_d}\n",
        "   \\end{bmatrix} \n",
        "   =\n",
        "   \\underset{(d+1) \\times 1}\n",
        "   {\n",
        "   \\begin{bmatrix}\n",
        "   -2\\sum_{i=1}^{n} x^i_0 \\left( y^i - wx^i \\right) \\\\\n",
        "   -2\\sum_{i=1}^{n} x^i_1 \\left( y^i - wx^i \\right) \\\\\n",
        "   \\vdots \\\\\n",
        "   -2\\sum_{i=1}^{n} x^i_d \\left( y^i - wx^i \\right) \\\\\n",
        "   \\end{bmatrix}\n",
        "   }\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks like we might be able to apply our fourth point ($Xw$, but\n",
        "in this case $w$ is $(y - Xw)$. But unlike our fourth point,\n",
        "we now sum along data points ($n$) instead of dimensions\n",
        "($d$). For this, we want each row of $X$ to be one given\n",
        "dimension along all data points instead of one data point with all\n",
        "dimensions, and thus we use $X^T$ instead of $X$. Finally,\n",
        "here’s the full derivative in matrix notation:\n",
        "\n",
        "\\begin{align}\\nabla L_w = -2X^T(y-Xw)\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting to zero and solve:\n",
        "\n",
        "\\begin{align}\\begin{align}\n",
        "   0 &= -2X^T(y-Xw) \\\\\n",
        "   &= X^T(y-Xw)     \\\\ \n",
        "   &= X^Ty - X^TXw \n",
        "   \\end{align}\\end{align}\n",
        "\n",
        "Move $X^TX$ to other side and we get a closed-form solution:\n",
        "\n",
        "\\begin{align}\\begin{align}\n",
        "   X^TXw &= X^Ty    \\\\\n",
        "   w &= (X^TX)^{-1}X^Ty\n",
        "   \\end{align}\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In NumPy, this is:\n",
        "\n",
        ".. code:: python\n",
        "\n",
        "   w = np.linalg.inv(X.T @ X) @ X @ y\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A NumPy Example\n",
        "---------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will create a fake dataset from the underlying equation\n",
        "$y = 2x + 7$:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def true_target(x):\n",
        "  return 2*x + 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In practical settings, there is no way we know this exact equation. We\n",
        "only get **observed** targets, and there’s some **noise** on it. The\n",
        "reason is that it’s impossible to measure any data out there in the\n",
        "world perfectly:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def observed_target(x):\n",
        "  \"\"\"Underlying data with Gaussian noise added\"\"\"\n",
        "  normal_noise = np.random.normal() * 3\n",
        "  return true_target(x) + normal_noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating data points\n",
        "~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, make 50 data points, observations and targets:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = 50\n",
        "\n",
        "# Features, X is [1,50]\n",
        "X = np.random.rand(N).reshape(N, 1) * 10\n",
        "\n",
        "# Observed targets\n",
        "y = np.array([observed_target(x) for x in X]).reshape(N, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding dummy dimension term to each $x^i$:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Append 1 for intercept term later\n",
        "X = np.hstack([np.ones((N, 1)), X])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that it **doesn’t matter** here whether we add it to the front or\n",
        "back, it will simply reflect correspondingly in our solution $w$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize our data points with respect to the true line\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For plotting\n",
        "features = X[:,1:] # exclude the intercept for plotting\n",
        "target = y\n",
        "true_targets = true_target(X[:,1:])\n",
        "\n",
        "plt.scatter(features, target, s=10, label='Observed data points')\n",
        "plt.plot(features, true_targets, c='blue', label='True target line y = 2x + 7', alpha=0.3)\n",
        "\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.legend(loc='best')\n",
        "plt.title('True and observed data points')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute a closed-form solution\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our goal is to get the line that is closest to that true target (blue)\n",
        "line as possible, without the knowledge of its existence. For this we\n",
        "use linear regression to fit observed data points by following the\n",
        "formula from the previous section:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "w = np.linalg.inv(X.T @ X) @ X.T @ y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To predict, we compute $\\hat{y} = xw$ for each data point\n",
        "$x^i$. Here we predict the training set (``X``) itself:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted = X @ w # y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To predict a set of new points, you just make it the same format as\n",
        "``X``, e.g., rows of data points.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize best fit line vs. true target line\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.scatter(features, target, s=10, label='Data points')\n",
        "plt.plot(features, true_targets, c='blue', label='True target line', alpha=0.3)\n",
        "plt.plot(features, predicted, c='red', label='Best fit line')\n",
        "\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That’s pretty close.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding the result\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "And our $w$ is:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we append ones in front of each data point $x$, ``w[0]``\n",
        "will be the intercept term and ``w[1]`` will be the slope. So our\n",
        "predicted line will be in the format of ``y = w[1]*x + w[0]``. Recall\n",
        "the *true* equation $y = 2x + 7$, you can see that we almost got\n",
        "the true slope (2):\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Our slope is\", w[1][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The intercept seems a little off, but that’s okay because our data is in\n",
        "a big range ($x \\in [0, 50], y \\in [7, 107]$). If we normalize the\n",
        "data into $[0, 1]$ range, expect it to be much closer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is our sum of squared error for the best fit line. Note that the\n",
        "number doesn’t mean anything much, apart from that this is the least\n",
        "possible loss we would get from any lines that try to fit the data:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "diff = (y - X @ w)\n",
        "loss = diff.T @ diff\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you don’t want intermediate variable, you can use ``np.linalg.norm``,\n",
        "but to get the sum of squared loss, you have to square that after:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = np.linalg.norm(y - X @ w, ord=2) ** 2\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the loss surface\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "Let’s confirm that our solution is really the one with lowest loss by\n",
        "seeing the loss surface.\n",
        "\n",
        "Our loss function $L(w)$ depends on two dimensions of $w$,\n",
        "e.g., ``w[0]`` and ``w[1]``. If we plot $L(w)$ over possible\n",
        "values of ``w[0]`` and ``w[1]``, the minimum of $L(w)$ should be\n",
        "near ``w = [5.17,2.066]``, which is our solution.\n",
        "\n",
        "To plot that out, first we have to create all possible values of w[0]\n",
        "and w[1] in a grid.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Ranges of w0 and w1 to see, centering at the true line\n",
        "spanning_radius = 10\n",
        "w0range = np.arange(7-spanning_radius, 7+spanning_radius, 0.05)\n",
        "w1range = np.arange(2-spanning_radius, 2+spanning_radius, 0.05)\n",
        "w0grid, w1grid = np.meshgrid(w0range, w1range)\n",
        "\n",
        "range_len = len(w0range)\n",
        "print(\"Number of values in each axis:\", range_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This means we’ll look into a total of 400*400 = 160,000 values of ``w``.\n",
        "We have to calculate loss for each pair of ``w0, w1``:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Make [w0, w1] in (2, 14400) shape\n",
        "all_w0w1_values = np.hstack([w0grid.flatten()[:,None], w1grid.flatten()[:,None]]).T\n",
        "\n",
        "# Compute all losses, reshape back to grid format\n",
        "all_losses = (np.linalg.norm(y - (X @ all_w0w1_values), axis=0, ord=2) ** 2).reshape((range_len, range_len))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can plot the loss surface (with minimum at the red point):\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,6))\n",
        "ax = fig.gca(projection='3d')\n",
        "\n",
        "ax.plot_surface(w0grid, w1grid, all_losses, alpha=0.5, cmap='RdBu')\n",
        "ax.contour(w0grid, w1grid, all_losses, offset=0, alpha=1, cmap='RdBu')\n",
        "ax.scatter(w[0], w[1], loss, lw=3, c='red', s=100, label=\"Minimum point (5.9,2.2)\")\n",
        "\n",
        "ax.legend(loc='best')\n",
        "ax.set_xlabel('w[0]')\n",
        "ax.set_ylabel('w[1]')\n",
        "ax.set_zlabel('L(w)')\n",
        "ax.set_xticks(np.arange(7-spanning_radius, 7+spanning_radius, 2))\n",
        "ax.set_yticks(np.arange(2-spanning_radius, 2+spanning_radius, 2))\n",
        "ax.set_zticks([loss])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can notice the bowl **centers** at the solution.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using sklearn\n",
        "-------------\n",
        "\n",
        "Using ``sklearn`` for linear regression is very simple (if you already\n",
        "understand all the concepts above).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we create the classifier ``clf``. If ``fit_intercept`` is ``True``\n",
        "(default), then it adds the dummy ‘1’ to the ``X``. But we already did\n",
        "that manually, so set it to ``False`` here.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = LinearRegression(fit_intercept=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then fit the data:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf.fit(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the $w$ learned, it’s the same as ours:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(clf.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Previously we use ``X @ w`` to predict data. For ``sklearn`` we can use\n",
        "``clf.predict``:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted = clf.predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the result is the same:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.scatter(X[:,1:], y, s=10, label='Data points')\n",
        "plt.plot(X[:,1:], true_targets, c='blue', label='True line', alpha=0.3)\n",
        "plt.plot(X[:,1:], predicted, c='red', label='Best fit line')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
